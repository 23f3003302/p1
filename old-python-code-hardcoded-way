import subprocess
from fastapi import FastAPI, Query,Body,HTTPException
from collections import Counter
import os
import uvicorn
import openai
import json
from pydantic import BaseModel
import datetime
import sqlite3
import duckdb
from git import Repo
import markdown
import whisper
import pandas as pd
from PIL import Image
import pytesseract

DEBUG=True
def debug(*str): print(); DEBUG and print(*str)  ; print()

task_description = """
A1. Install uv (if required) and run {https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py }with ${user.email} as the only argument. (NOTE: This will generate data files required for the next tasks.)
A2. Format the contents of {/data/format.md} using {prettier}@{3.4.2 } , updating the file in-place
A3. The file {/data/dates.txt} contains a list of dates, one per line. Count the number of {Wednesdays} in the list, and write just the number to {/data/dates-wednesdays.txt}
A4. Sort the array of contacts in {/data/contacts.json} by {last_name, then first_name}, and write the result to {/data/contacts-sorted.json}
A5. Write the first line of the {10} most recent .log file in {/data/logs/} to {/data/logs-recent.txt}, most recent first
A6. Find all Markdown (.md) files in {/data/docs/}. For each file, extract the first occurrence of each H1 (i.e. a line starting with # ). Create an index file {/data/docs/index.json} that maps each filename (without the /data/docs/ prefix) to its title (e.g. {"README.md": "Home", "path/to/large-language-models.md": "Large Language Models", ...})
A7. {/data/email.txt} contains an email message. Pass the content to an LLM with instructions to extract the sender’s email address, and write just the email address to {/data/email-sender.txt}
A8. {/data/credit-card.png} contains a credit card number. Pass the image to an LLM, have it extract the card number, and write it without spaces to {/data/credit-card.txt}
A9. {/data/comments.txt} contains a list of comments, one per line. Using embeddings, find the most similar pair of comments and write them to {/data/comments-similar.txt}
A10. The SQLite database file {/data/ticket-sales.db} has a tickets with columns type, units, and price. Each row is a customer bid for a concert ticket. What is the total sales of all the items in the {“Gold”} ticket type? Write the number in /{data/ticket-sales-gold.txt}
B3. Fetch data from an API and save it
B4. Clone a git repo and make a commit
B5. Run a SQL query on a SQLite or DuckDB database
B6. Extract data from (i.e. scrape) a website
B7. Compress or resize an image
B8. Transcribe audio from an MP3 file
B9. Convert Markdown to HTML
B10. Write an API endpoint that filters a CSV file and returns JSON data
"""

#use my api key here
openai.api_key = ''

app = FastAPI()

def parse_task_description(description, instruction_text):
    prompt = f"""You are an expert in task classification and argument extraction. Given the following predefined automation tasks:
    {instruction_text}
    Identify the most relevant task ID for the provided task description and extract key arguments needed to execute it.
    Return only JSON 1-liner string in the format {{'task': 'A#', 'parameters': {{'key1': 'value1', 'key2': 'value2'}}}}.
    Task description: {description}
    """
    #stored in task description's paranthesis
    
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=400
    )
    
    return json.loads(response['choices'][0]['message']['content'].strip().replace("```json\n", "").replace("\n```", "").replace("'", '"'))

def findsimilarembeddings(instruction_text):
    prompt = f"""Below contains a list of comments, one per line:
    {instruction_text}
    Using embeddings, find the most similar pair of comments and write only those 2 lines in JSON 1-liner string in the format {{'s1':'put line 1 here','s2':'put line 2 here'}}.
    """
    
    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=400
    )
    
    return json.loads(response['choices'][0]['message']['content'].strip().replace("```json\n", "").replace("\n```", "").replace("'", '"'))


class TaskRequest(BaseModel):
    task: str | None = None

@app.post("/run")
def run_prettier(task_query: str | None = Query(None, alias="task"), 
        task_body: TaskRequest | None = Body(None)):
    instruction_text =  task_query or (task_body.task if task_body else None)
    if instruction_text is None:
        raise HTTPException(status_code=500, detail=f"Error happened : {'task not given'}")

    debug('post call : ',instruction_text)
    debug("open api token ",os.environ['AIPROXY_TOKEN'] )
    
    # try:        
    response = parse_task_description(task_description, instruction_text)
    debug('chatgpt Processing Task Description  ................ ')
    debug('chatgpt o/p : ',response)
    task_id = response['task']
    parameters = response['parameters']
    debug('task id : ',task_id); debug('parameters : ',parameters)

    if task_id == "A1":
    # {'task': 'A1', 'parameters': {'url': 'https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py', 'email': '${user.email}'}}
        
        #subprocess.run(["pip", "install", "uv"], check=True)
        values = list(parameters.values())
        url = values[0]
        email = values[1]

        import requests
        import subprocess

        # url = "https://raw.githubusercontent.com/sanand0/tools-in-data-science-public/tds-2025-01/project-1/datagen.py"
        response = requests.get(url)

        script_path = "datagen.py"
        with open(script_path, "w") as file:
            file.write(response.text)

        # Run script with email argument
        result = subprocess.run(["python", script_path, email], capture_output=True, text=True)

        debug("A1.TASK done processing, post results o/p: ")

        debug("STDOUT Captured Output:\n", result.stdout)
        debug("STDOUT Captured Errors:\n", result.stderr)

    elif task_id == "A2":
    # {'task': 'A2', 'parameters': {'file': '/data/format.md', 'formatter': 'prettier', 'version': '3.4.2'}}
        values = list(parameters.values())
        file = values[0]
        tool = values[1]      
        version = values[2]  

        import subprocess

        # Step 1: Install the specific version of Prettier locally (inside the user's environment)
        # subprocess.run(["npm", "install", f"{tool}@{version}"], check=True)

        # Step 2: Ensure `npx` runs the installed version of Prettier
        result = subprocess.run(
            ["npx", "--yes", f"{tool}@{version}", "--write", file], 
            check=True, capture_output=True, text=True
        )

        debug("STDOUT Prettier Output:", result.stdout)
        debug("STDOUT Prettier Errors:", result.stderr)
        
    elif task_id == "A3":
    # {'task': 'A3', 'parameters': {'file_input': '/data/dates.txt', 'day': 'Wednesdays', 'file_output': '/data/dates-wednesdays.txt'}}
    
        values = list(parameters.values())
        input_file = values[0]
        day = values[1].strip().lower()  # Normalize input day to lowercase
        output_file = values[2]

        # Map weekday names (including variations) to their corresponding indices
        weekday_map = {
            'monday': 0,
            'tuesday': 1,
            'wednesday': 2,
            'thursday': 3,
            'friday': 4,
            'saturday': 5,
            'sunday': 6
        }

        # Normalize day input: Handle variations like 'wed', 'wednesdays', etc.
        if 'mon' in day:
            day_index = weekday_map['monday']
        elif 'tue' in day:
            day_index = weekday_map['tuesday']
        elif 'wed' in day:
            day_index = weekday_map['wednesday']
        elif 'thu' in day:
            day_index = weekday_map['thursday']
        elif 'fri' in day:
            day_index = weekday_map['friday']
        elif 'sat' in day:
            day_index = weekday_map['saturday']
        elif 'sun' in day:
            day_index = weekday_map['sunday']
        else:
            return {"status": "Error", "message": f"Invalid day format: {day}"}

        # Open and read the input file
        with open(input_file, 'r') as f:
            dates = [line.strip() for line in f.readlines()]

        # List of possible date formats to try
        date_formats = [
            "%Y/%m/%d %H:%M:%S",  # e.g., '2000/06/29 01:33:37'
            "%b %d, %Y",          # e.g., 'Jun 01, 2005'
            "%Y-%m-%d",           # e.g., '2005-06-01'
            "%d-%b-%Y",           # e.g., '27-Sep-2003'
        ]

        def parse_date(date_string):
            for fmt in date_formats:
                try:
                    return datetime.datetime.strptime(date_string, fmt)
                except ValueError:
                    continue  # Try the next format if this one fails
            raise ValueError(f"Unable to parse date: {date_string}")


        # Count weekdays using the `parse_date` function to handle multiple formats
        weekday_counts = Counter(parse_date(d).weekday() for d in dates)

        
        # Write the count to the output file
        with open(output_file, 'w') as f:
            f.write(str(weekday_counts[day_index]))

    elif task_id == "A4":
    # {'task': 'A4', 'parameters': {'input_file': '/data/contacts.json', 'output_file': '/data/contacts-sorted.json', 'sort_by': ['last_name', 'first_name']}}
        values = list(parameters.values())
        input_file = values[0]
        output_file = values[1]
        sort_fields = values[2]

        # Prepare the jq sorting fields correctly
        # eg: ['last_name', 'first_name'] should become '.last_name, .first_name'
        sort_expression = ', '.join([f".{field}" for field in sort_fields])
        
        debug(input_file,output_file,sort_expression)

        cmd = f"jq 'sort_by({sort_expression})' {input_file} > {output_file}"
        debug('A4: shell cmd is : ',cmd)

        import subprocess
        subprocess.run(cmd, shell=True, check=True)

    elif task_id == "A5":
        values = list(parameters.values())
        num_files = values[0]
        input_dir = values[1]
        output_file = values[2]
    
        import subprocess
        cmd = f"ls -t {input_dir}*.log | head -n {num_files} | xargs -d '\\n' -I {{}} sh -c 'head -n 1 \"{{}}\"' > {output_file}"
        subprocess.run(cmd, shell=True, check=True)
    
    elif task_id == "A6":
    # {'task': 'A6', 'parameters': {'directory': '/data/docs/', 'output_file': '/data/docs/index.json'}}
            
        values = list(parameters.values())
        input_dir = values[0]
        output_file = values[1]     
        
        import subprocess
        subprocess.run(f"""
            echo "{{" > {output_file} && 
            find /data/docs/ -name "*.md" -exec bash -c 'filename="{{}}"; title=$(grep -m 1 "^#" {{}} | sed "s/^# //"); echo -e "  \\"${{filename##*/}}\\": \\"$title\\"," >> {output_file}' \\; && 
            sed -i '$ s/,$//' {output_file} && 
            echo "}}" >> {output_file}
            """, shell=True, check=True)

# echo "{" > /data/docs/index.json
# find /data/docs/ -name "*.md" -exec bash -c 'filename="{}"; title=$(grep -m 1 "^#" {} | sed "s/^# //"); echo -e "  \"${filename##*/}\": \"$title\"," >> /data/docs/index.json' \;
# sed -i '$ s/,$//' /data/docs/index.json
# echo "}" >> /data/docs/index.json

    elif task_id == "A7":
        #seems like re is not doing job for complex, use LLM instead
        values = list(parameters.values())
        input_file = values[0]
        output_file = values[1]    


        with open(input_file, 'r') as f:
            email_content = f.read()

        # match = re.search(r'From: (.+@.+\..+)', email_content)
        # email = match.group(1) if match else "Not found"
        # with open(output_file, 'w') as f:
        #     f.write(email)



        # Define the prompt for the LLM
        prompt = f"""
        Extract the sender's email address from the following email message. 
        Return only the email address, nothing else.

        Email content:
        {email_content}
        """            
    
        # Call the LLM (Assuming OpenAI API)
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10
        )

        # Extract the email from the response
        extracted_email = response["choices"][0]["message"]["content"].strip()

        # Write the extracted email to the output file
        with open(output_file, "w", encoding="utf-8") as file:
            file.write(extracted_email)

        debug("A7: Extracted sender email:", extracted_email)        
    
    elif task_id == "A8":
        values = list(parameters.values())
        input_image = values[0]
        output_file = values[1]     

        # # Convert image to base64
        # with open(input_image, "rb") as img_file:
        #     base64_image = base64.b64encode(img_file.read()).decode("utf-8")

        # # Send to LLM for processing
        # response = openai.ChatCompletion.create(
        #     model="gpt-4o-mini",
        #     messages=[
        #         {"role": "system", "content": "Extract the credit card number from the provided image."},
        #         {"role": "user", "content": f"Here is an image containing a credit card number: {base64_image}"}
        #     ]
        # )

        # # Extract the response text
        # extracted_text = response["choices"][0]["message"]["content"]
        # debug('extracted_text: ',extracted_text)

        # # Find the first valid credit card number format
        # card_number = re.sub(r'\D', '', extracted_text)  # Remove non-numeric characters

        # if not card_number:
        #     pass
        # else :
            # Load the image

        # LLM sometimes works, sometimes doesnt, so skip it and use tesseract

        image = Image.open(input_image)

        # Extract text using OCR
        text = pytesseract.image_to_string(image)

        # Extract only the credit card number (digits only, removing spaces)
        import re
        card_number = "".join(re.findall(r"\d+", text))

        print("Extracted Credit Card Number:", card_number)


        # Save the extracted card number
        with open(output_file, "w") as f:
            f.write(card_number)

        debug("A8: Card number extracted and saved successfully : ", card_number)

    elif task_id == "A9":
        values = list(parameters.values())
        input_file = "/data/comments.txt" #values[0]
        output_file = "/data/comments-similar.txt" # values[1]         



        # Read comments from file
        with open(input_file, "r") as f:
            comments = [line.strip() for line in f.readlines() if line.strip()]


        response = findsimilarembeddings(comments)
        debug(response)
        best_pair = [response['s1'],response['s2']]

        debug('A9 best pair ... : \n ',best_pair)

        # Write the most similar comments to file
        with open(output_file, "w") as f:
            f.write(best_pair[0] + "\n")
            f.write(best_pair[1] + "\n")


    elif task_id == "A10":
        values = list(parameters.values())
        input_file = values[0]
        ticket_type = values[1]     
        output_file = values[2]     

        

        db_path = input_file
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # Query to calculate total sales for "Gold" tickets
        cursor.execute("SELECT SUM(units * price) FROM tickets WHERE type = ?", (ticket_type,))

        total_sales = cursor.fetchone()[0]

        # Ensure total_sales is not None (if no Gold tickets exist, default to 0)
        total_sales = total_sales if total_sales is not None else 0

        # Write the result to a file
        with open(output_file, "w") as file:
            file.write(str(total_sales))

        # Close the database connection
        conn.close()

        print("Total sales for Gold tickets:", total_sales)


    elif task_id == "B3":
        values = list(parameters.values())
        url = values[0]
        output = values[1]

        import requests
        response = requests.get(url)

        with open(output, "w") as file:
            file.write(response.text)

    elif task_id == "B4":
        values = list(parameters.values())
        repo_url = values[0]

        repo_dir = '/data/repo'
        repo = Repo.clone_from(repo_url, repo_dir)
        repo.git.commit("-am", 'B4 commit_message')


    elif task_id == "B5":
        values = list(parameters.values())
        db_path = values[0]
        query = values[1]
        output_file = values[2]

        conn = sqlite3.connect(db_path) if db_path.endswith(".sqlite") else duckdb.connect(db_path)
        cursor = conn.cursor()
        cursor.execute(query)
        result = cursor.fetchall()
        conn.close()
        with open(output_txt, "w") as f:
            f.write(result)        
        return result
    
    elif task_id == "B6":
        values = list(parameters.values())
        url = values[0]
        output_txt = values[1]

        import requests
        response = requests.get(url)
        with open(output_txt, "w") as f:
                f.write(response.text)

    elif task_id == "B7":
        values = list(parameters.values())
        input_file = values[0]
        size = (values[1],values[2])

        img = Image.open(input_file)
        img = img.resize(size)
        img.save(output_file)        

    elif task_id == "B8":
        values = list(parameters.values())
        mp3_file = values[0]
        output_txt = values[1]

        model = whisper.load_model("base")
        result = model.transcribe(mp3_file)
        with open(output_txt, "w") as f:
            f.write(result["text"])

    elif task_id == "B9":
        values = list(parameters.values())
        md_file = values[0]
        html_file = values[1]

        with open(md_file, "r") as f:
            html = markdown.markdown(f.read())
        with open(html_file, "w") as f:
            f.write(html)

    elif task_id == "B10":
        values = list(parameters.values())
        input_csv = values[0]
        filter_column = values[1]
        filter_value = values[2]
        output_file = values[3]

        df = pd.read_csv(input_csv)
        df_filtered = df[df[filter_column] == filter_value]
        with open(output_file, "w") as f:
            f.write(df_filtered.to_json())
        return df_filtered.to_json()
            
    return {"message": "Hello, World!"}
    # except Exception as e:
    #     raise HTTPException(status_code=500, detail=f"Error happened : {e.stderr}")


@app.get("/read")
def read_file(task_query: str | None = Query(None, alias="path"), 
        task_body: TaskRequest | None = Body(None)):
    path =  task_query or (task_body.task if task_body else None)
    if path is None:
        raise HTTPException(status_code=500, detail=f"Error happened : {'path not given'}")
    try:
        result = subprocess.run(["prettier", "--parser", "markdown", path], capture_output=True, text=True, check=True)
        returnval = {"path": path, "formatted_content": result.stdout}
        returnval = result.stdout
        debug (returnval)
        return returnval
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error happened : {e.stderr}")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
